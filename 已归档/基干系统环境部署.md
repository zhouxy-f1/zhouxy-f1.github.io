## 第一章 环境简介

##### 控制中心测试环境

![1111111](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqg0s0hm4j30n507sjs0.jpg)

| IP          | 主机名   | 安装软件                                                |
| ----------- | -------- | ------------------------------------------------------- |
| 10.1.20.125 | server6  | GlusterFS、Kafka、K8S-master、PostgreSQL、Pgpool、Redis |
| 10.1.20.181 | server7  | GlusterFS、Kafka、K8S-node1、 PostgreSQL、Pgpool        |
| 10.1.20.242 | server8  | GlusterFS、Kafka、K8S-node2                             |
| 10.1.20.210 | server9  | GlusterFS                                               |
| 10.1.20.249 | server10 | none                                                    |

##### 操作系统初始化

```bash
#k8s安装环境初始化，在server6、7、8上都要执行

1.在不同主机上修改各自的主机名
hostnamectl set-hostname server6

2.关闭selinux、防火墙、swap、清空iptables
setenforce 0
sed -i '/^SELINUX/cSELINUX=disabled' /etc/selinux/config
systemctl stop firewalld
systemctl disable firewalld	
iptables -F
swapoff -a
sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab

3.升级内核到4.4
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
yum --enablerepo=elrepo-kernel install -y kernel-lt
grub2-set-default "CentOS Linux (4.4.182-1.el7.elrepo.x86_64)7(Core)"
reboot

4.开机自动加载ipvs模块
cat>/etc/sysconfig/modules/ipvs.modules<<EOF
#!/bin/bash
ipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack"
for kernel_module in \${ipvs_modules}; do
  /usr/sbin/modinfo -F filename \${kernel_module} > /dev/null 2>&1
  if [ $? -eq 0 ]; then
    /sbin/modprobe \${kernel_module}
  fi
done
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs

5.开启路由转发
cat>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6talbes=1
net.ipv6.conf.all.disable_ipv6=1
vm.swappiness=0
EOF
sysctl -p /etc/sysctl.d/kubernetes.conf
```

## 第二章 Pgpool-II双机热备集群

##### linux8安装postgresql

```bash
#配置源并安装postgresql-11
dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm
dnf -qy module disable postgresql
dnf install -y postgresql11-server
```

##### 安装部署postgresql11

```bash
postgresql11需要在server6和server7上都安装

#下载安装postgresql
yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
yum install -y postgresql11 postgresql11-server postgresql11-contrib

#配置用户和目录
useradd postgres
mkdir -p /home/pgdata/pgsql/{data,log,archivedir}
chown -R postgres.postgres /home/pgdata/pgsql /usr/pgsql-11/
cat >>/etc/hosts << EOF
10.1.20.125  pg-test-1
10.1.20.181  pg-test-2
EOF

#数据库初始化
1.修改初始化脚本
sed -i '/^PREVDATADIR/cPREVDATADIR=/home/pgdata/pgsql' /usr/pgsql-11/bin/postgresql-11-setup
2.修改配置文件
sed -i 's#/var/lib/pgsql/11#/home/pgdata/pgsql#g' /usr/lib/systemd/system/postgresql-11.service
systemctl daemon-reload
3.初始化数据库
/usr/pgsql-11/bin/postgresql-11-setup initdb

#配置数据库访问控制
cat>/home/pgdata/pgsql/data/pg_hba.conf<<'EOF'
host    replication     replication     10.1.20.125/32   md5
host    replication     replication     10.1.20.181/32   md5
host    all             postgres        10.1.20.125/32   trust
host    all             postgres        10.1.20.181/32   trust
host    all             repl            10.1.20.125/32   trust
host    all             repl            10.1.20.181/32   trust
EOF

#修改配置文件
vim /home/pgdata/pgsql/data/postgresql.conf
第59行	listen_addresses = '*'
第63行	port = 5433
第64行	max_connections = 500
第122行	max_prepared_transactions = 900
第184行	wal_level = hot_standby
第221行	archive_mode = on
第223行	archive_command = 'cp "%p" "/home/pgdata/pgsql/archivedir/%f"'
第239行	max_wal_senders = 3
第241行	wal_keep_segments = 32
第244行	max_replication_slots = 3
第263行	hot_standby = on
第273行	hot_standby_feedback = on
末行添加	pgpool.pg_ctl = '/usr/pgsql-11/bin/pg_ctl'

#启动数据库服务
systemctl start postgresql-11
systemctl enable postgresql-11
```

##### 安装部署Pgpool-II

```bash
pgpool-II是一个集连接池、主备切换与负载均衡于一身的数据库连接件,需要在server6和server7上都安装

yum install -y http://www.pgpool.net/yum/rpms/4.0/redhat/rhel-7-x86_64/pgpool-II-release-4.0-1.noarch.rpm
yum install -y pgpool-II-pg11*
yum install -y postgresql-devel

#拷贝pgpool-II配置文件（两台都执行）
cd  /var/lib/pgsql/
curl -O https://www.pgpool.net/mediawiki/images/pgpool-II-4.0.6.tar.gz
tar -xf pgpool-II-4.0.6.tar.gz
curl -O https://www.pgpool.net/mediawiki/images/pgpool-II-3.5.2.tar.gz
tar -xf pgpool-II-3.5.2.tar.gz
mkdir /home/pgdata/pgsql/data/sql
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/insert_lock.sql /home/pgdata/pgsql/data/sql/
cp /var/lib/pgsql/pgpool-II-3.5.2/src/sql/pgpool_adm/pgpool_adm.sql.in /home/pgdata/pgsql/data/sql/pgpool_adm.sql
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-recovery/pgpool-recovery.sql.in /home/pgdata/pgsql/data/sql/pgpool-recovery.sql
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-recovery/uninstall_pgpool-recovery.sql /home/pgdata/pgsql/data/sql/
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-regclass/pgpool-regclass.sql.in /home/pgdata/pgsql/data/sql/pgpool-regclass.sql
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-regclass/uninstall_pgpool-regclass.sql /home/pgdata/pgsql/data/sql/
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool_adm/pgpool_adm.control /etc/pgpool-II/
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool_adm/pgpool_adm--1.0.sql /etc/pgpool-II/
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-recovery/pgpool_recovery.control /etc/pgpool-II/
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-recovery/pgpool_recovery--1.1.sql /etc/pgpool-II/
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-regclass/pgpool_regclass.control /etc/pgpool-II/
cp /var/lib/pgsql/pgpool-II-4.0.6/src/sql/pgpool-regclass/pgpool_regclass--1.0.sql /etc/pgpool-II/
rm -fr /var/lib/pgsql/pgpool-II-4.0.6
rm -f /var/lib/pgsql/pgpool-II-4.0.6.tar.gz
rm -fr /var/lib/pgsql/pgpool-II-3.5.2
rm -f /var/lib/pgsql/pgpool-II-3.5.2.tar.gz
sed -i "s/MODULE_PATHNAME/\/usr\/pgsql-11\/lib\/pgpool_adm/g" /etc/pgpool-II/pgpool_adm--1.0.sql
sed -i "s/MODULE_PATHNAME/\/usr\/pgsql-11\/lib\/pgpool_adm/g" /home/pgdata/pgsql/data/sql/pgpool_adm.sql
sed -i "s/MODULE_PATHNAME/\/usr\/pgsql-11\/lib\/pgpool-recovery/g" /etc/pgpool-II/pgpool_recovery--1.1.sql
sed -i "s/MODULE_PATHNAME/\/usr\/pgsql-11\/lib\/pgpool-recovery/g" /home/pgdata/pgsql/data/sql/pgpool-recovery.sql
chown postgres.postgres -R /home/pgdata/pgsql/data/sql

#分别修改两台数据库的配置文件pgpool.conf
1.在server6上执行
vim /etc/pgpool-II/pgpool.conf
listen_addresses = '*'
port = 5432
backend_hostname0 = 'pg-test-1'
backend_port0 = 5433
backend_data_directory0 = '/home/pgdata/pgsql/data'
backend_hostname1 = 'pg-test-2'
backend_port1 = 5433
backend_weight1 = 1
backend_data_directory1 = '/home/pgdata/pgsql/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'
enable_pool_hba = on
num_init_children = 400
max_pool = 10
log_destination = 'syslog'
syslog_facility = 'LOCAL1'
load_balance_mode = on
master_slave_mode = on
sr_check_period = 5
sr_check_user = 'postgres'
sr_check_password = 'test'
health_check_period = 5
health_check_timeout = 30
health_check_user = 'postgres'
health_check_password = 'test'
health_check_max_retries = 3
failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R'
recovery_user = 'postgres'
recovery_password = 'test'
recovery_1st_stage_command = 'recovery_1st_stage.sh'
use_watchdog = on
trusted_servers = 'pg-test-1,pg-test-2'
wd_hostname = 'pg-test-1'
wd_priority = 2
delegate_IP = '10.1.20.199'
if_cmd_path = '/bin'
if_up_cmd = 'ip_w addr add $_IP_$/24 dev eth0 label eth0:0'
if_down_cmd = 'ip_w addr del $_IP_$/24 dev eth0'
arping_path = '/usr/bin'
arping_cmd = 'arping_w -U $_IP_$ -w 1'
wd_interval = 3
heartbeat_destination0 = 'pg-test-2'
other_pgpool_hostname0 = 'pg-test-2'
other_pgpool_port0 = 5432
other_wd_port0 = 9000

2.在server7上执行
vim /etc/pgpool-II/pgpool.conf
listen_addresses = '*'
port = 5432
backend_hostname0 = 'pg-test-1'
backend_port0 = 5433
backend_data_directory0 = '/home/pgdata/pgsql/data'
backend_hostname1 = 'pg-test-2'
backend_port1 = 5433
backend_weight1 = 1
backend_data_directory1 = '/home/pgdata/pgsql/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'
enable_pool_hba = on
num_init_children = 400
max_pool = 10
log_destination = 'syslog'
syslog_facility = 'LOCAL1'
load_balance_mode = on
master_slave_mode = on
sr_check_period = 5
sr_check_user = 'postgres'
sr_check_password = 'test'
health_check_period = 5
health_check_timeout = 30
health_check_user = 'postgres'
health_check_password = 'test'
health_check_max_retries = 3
failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R'
recovery_user = 'postgres'
recovery_password = 'test'
recovery_1st_stage_command = 'recovery_1st_stage.sh'
use_watchdog = on
trusted_servers = 'pg-test-1,pg-test-2'
wd_hostname = 'pg-test-2'
wd_priority = 2
delegate_IP = '10.1.20.199'
if_cmd_path = '/bin'
if_up_cmd = 'ip_w addr add $_IP_$/24 dev eth0 label eth0:0'
if_down_cmd = 'ip_w addr del $_IP_$/24 dev eth0'
arping_path = '/usr/bin'
arping_cmd = 'arping_w -U $_IP_$ -w 1'
wd_interval = 3
heartbeat_destination0 = 'pg-test-1'
other_pgpool_hostname0 = 'pg-test-1'
other_pgpool_port0 = 5432
other_wd_port0 = 9000

#两台机器都要修改对pgpool-II的访问控制
cat >>/etc/pgpool-II/pool_hba.conf <<EOF
host    all         all         0.0.0.0/0             md5
host    all         all         0/0                   md5
EOF

#配置Pgpool-II日志输出
mkdir /var/log/pgpool-II
touch /var/log/pgpool-II/pgpool.log
cat>>/etc/rsyslog.conf<<EOF
*.info;mail.none;authpriv.none;cron.none;LOCAL1.none    /var/log/messages
LOCAL1.*                                                /var/log/pgpool-II/pgpool.log
EOF
sed -i 1i/var/log/pgpool-II/pgpool.log /etc/logrotate.d/syslog
systemctl restart rsyslog

#修改pcp.conf
pg_md5 passwordpcp														#得出结果是：	c05c1fdd77f01ce13a91981321c442
echo 'pgpool:33c05c1fdd77f01ce13a91981321c442' >>/etc/pgpool-II/pcp.conf
pg_md5 -f /etc/pgpool-II/pgpool.conf -m -u postgres test		#这里是postgres连接vip的密码

#配置非管理员执行ip和arping(两台机器都执行)
cat >>/etc/sudoers <<EOF
postgres ALL=(root) NOPASSWD: /sbin/ip
www-data ALL=(root) NOPASSWD: /sbin/ip
postgres ALL=(root) NOPASSWD: /usr/sbin/arping
www-data ALL=(root) NOPASSWD: /usr/sbin/arping
EOF
```

##### 配置集群高可用

```bash
#新建恢复配置文件
1.在primary上执行
cat> /home/pgdata/pgsql/data/recovery.done<<EOF
standby_mode = 'on'
primary_conninfo = 'user=replication password="passwordtest" host=10.1.20.125 port=5433 sslmode=prefer sslcompression=0 krbsrvname=postgres target_session_attrs=any'
EOF
2.在standby上执行
cat >/home/pgdata/pgsql/data/recovery.conf <<EOF
standby_mode = 'on'
primary_conninfo = 'user=replication password="passwordtest" host=10.1.20.181 port=5433 sslmode=prefer sslcompression=0 krbsrvname=postgres target_session_attrs=any'
EOF

#设置恢复账户（仅在primary上执行）
su - postgres
psql -p 5433
CREATE ROLE replication WITH REPLICATION PASSWORD 'passwordtest' LOGIN;
CREATE ROLE repl WITH REPLICATION PASSWORD 'yourReplPasswords' LOGIN;
ALTER USER postgres WITH PASSWORD 'test';
\q

#创建Recover和Admin方法（仅在primary上执行）
cd /home/pgdata/pgsql/data/sql
sudo -u postgres psql -h 10.1.20.125 -p 5433 -f pgpool-recovery.sql template1
vim /home/pgdata/pgsql/data/sql/pgpool_adm.sql
CREATE FUNCTION pcp_node_count(text, integer, text, text, OUT node_count integer)
RETURNS integer
CREATE FUNCTION pcp_node_count(text, OUT node_count integer)
RETURNS integer
sudo -u postgres psql -h 10.1.20.125 -p 5433  -f pgpool_adm.sql template1

#创建FailOver脚本（两台机器都执行）
vim /etc/pgpool-II/failover.sh
chown postgres.postgres /etc/pgpool-II/failover.sh
chmod 0700 /etc/pgpool-II/failover.sh

#创建恢复脚本（两台机器都执行）
vim /home/pgdata/pgsql/data/recovery_1st_stage.sh
chown -R postgres.postgres /home/pgdata/pgsql/data/recovery_1st_stage.sh
chmod +x /home/pgdata/pgsql/data/recovery_1st_stage.sh

#创建ip_w脚本（都执行）
vim /bin/ip_w
chmod +x /bin/ip_w 

#配置集群间免密（都执行）
passwd postgres  <= N)Zc)Dg4
cd ~/.ssh
ssh-keygen -t rsa -f id_rsa_pgpool
ssh-copy-id -i id_rsa_pgpool.pub postgres@pg-test-1
ssh-copy-id -i id_rsa_pgpool.pub postgres@pg-test-2
su - postgres
cd ~/.ssh
ssh-keygen -t rsa -f id_rsa_pgpool
ssh-copy-id -i id_rsa_pgpool.pub postgres@pg-test-1
ssh-copy-id -i id_rsa_pgpool.pub postgres@pg-test-2

#启动pgpool服务（都执行）
systemctl start pgpool.service
systemctl enable pgpool.service
```

##### 集群连接

```bash
sudo -u postgres psql -h 10.1.20.200 -p 5432			<=输入密码：test
```

## 第三章 安装部署Redis

##### 安装部署

```bash
redis作为缓存服务器只需要在server6上安装即可

#下载安装
yum install -y gcc automake autoconf libtool make							#如果没有C环境，需要提前安装
curl -O http://download.redis.io/releases/redis-5.0.8.tar.gz
tar xf redis-5.0.8.tar.gz -C /opt
ln -s /opt/redis-5.0.8/ /opt/redis
cd /opt/redis
make && make install

#配置redis
mkdir /etc/redis /home/redis/6379
cat>/etc/redis/6379.conf<<EOF
daemonize yes
bind 127.0.0.1 10.1.20.203
port 6379
dir /home/redis/6379
pidfile /var/run/redis_6379.pid
logfile /var/log/redis_6379.log
databases 16
requirepass test
EOF

#启动redis
cat >/usr/lib/systemd/system/redis.service<<EOF
[Unit]
Description=Redis persistent key-value database
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
ExecStart=/usr/local/bin/redis-server  /etc/redis/6379.conf --supervised systemd
ExecStop=/usr/local/bin/redis-cli -h 10.1.20.203  -p 6379 shutdown
Type=forking

[Install]
WantedBy=multi-user.target
EOF
systemctl daemon-reload
systemctl enable redis
systemctl start redis
systemctl status redis
```

## 第四章 部署GlusterFS

##### 环境简介

|     ip      | hostname |         角色          |
| :---------: | :------: | :-------------------: |
| 10.1.20.125 | server6  | gls001.longchuang.com |
| 10.1.20.181 | server7  | gls002.longchuang.com |
| 10.1.20.242 | server8  | gls003.longchuang.com |
| 10.1.20.210 | server9  | gls004.longchuang.com |

##### 安装部署

```bash
yum install -y centos-release-gluster
yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel
mkdir /home/glusterd
sed -i 's#/var/lib/glusterd#/home/glusterd#g' /etc/glusterfs/glusterd.vol
systemctl start glusterd.service
systemctl enable glusterd.service
cat >>/etc/hosts<<EOF
10.1.20.125 gls001.longchuang.com
10.1.20.181 gls002.longchuang.com
10.1.20.242 gls003.longchuang.com
10.1.20.210 gls004.longchuang.com
EOF

#把服务器加入存储池
1.在server6上执行
gluster peer probe gls002.longchuang.com
gluster peer probe gls003.longchuang.com
gluster peer probe gls004.longchuang.com

2.在server7上执行
gluster peer probe gls001.longchuang.com
gluster peer probe gls003.longchuang.com
gluster peer probe gls004.longchuang.com

3.在server8上执行
gluster peer probe gls001.longchuang.com
gluster peer probe gls002.longchuang.com
gluster peer probe gls004.longchuang.com

4.在server9上执行
gluster peer probe gls001.longchuang.com
gluster peer probe gls002.longchuang.com
gluster peer probe gls003.longchuang.com

#检查节点连接状态
gluster peer status
```

## 第五章 安装docker和harbor

##### 安装并启动docker

```bash
#安装启动
yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce-18.09.9
mkdir -p /home/lib/docker
mkdir /etc/docker
cat>/etc/docker/daemon.json<<EOF
{
        "graph": "/home/lib/docker",
        "registry-mirrors": ["https://registry.docker-cn.com"],
        "exec-opts":["native.cgroupdriver=systemd"]
}
EOF
systemctl start docker
systemctl enable docker

#验证是否正常运行，
docker run hello-world

当看到以下信息时说明docker配置正常
Hello from Docker!
This message shows that your installation appears to be working correctly...
```

##### 安装harbor私有仓库

```bash
当需要大规模使用docker hub时，大家都从官方仓库拉取镜像，非常占带宽，此时就需要私有仓库（registry），registry本就是一个服务，直接起用registry容器即可。但是通过registry配置私有仓库，功能弱，权限控制简单，删除镜像麻烦等问题，临时应急才用，一般选用harbor，harbor是把官方仓库加以拓展，功能性更强。harbor仓库只需要在服务器6上安装即可

#下载docker编排工具docker-compose
yum install -y docker-compose
docker-compose version

#自签证书
1.生成RSA私钥
openssl genrsa -des3 -out ca.key 2048

2.通过RSA私钥生成CSV证书签名请求
openssl req -new -key ca.key -out ca.csr					
注：需要依次输入私钥的密码、国家代码、省会、城市、组织、公司名等信息

3.删除私钥中的密码
openssl rsa -in ca.key -out ca.key

4.生成证书
openssl x509 -req -days 365 -in ca.csr -signkey ca.key -out harbor.longchuang.com.crt
openssl x509 -in harbor.longchuang.com.crt -out harbor.longchuang.com.pem -outform PEM
5.把证书拷贝到指定目录
mkdir /etc/docker/certs/ -p
mv ~/ca.key /etc/docker/certs/harbor.longchuang.com.key
mv ~/harbor.longchuang.com.crt /etc/docker/certs/

#下载harbor安装包并安装
wget https://github.com/goharbor/harbor/releases/download/v2.0.1/harbor-offline-installer-v2.0.1.tgz
tar xf harbor-offline-installer-v2.0.1.tgz  -C /opt
cp /opt/harbor/harbor.yml.tmpl /opt/harbor/harbor.yml
vim /opt/harbor/harbor.yml
第5行			hostname: harbor.longchuang.com
第10行		port: 8088
第17行		certificate: /etc/docker/certs/harbor.longchuang.com.crt
第18行		private_key: /etc/docker/certs/harbor.longchuang.com.key
  
cd /opt/harbor
./prepare
./install

#启动harbor服务并访问harbor站点
docker-compose -f /opt/harbor/docker-compose.yml  -d up
http://10.1.20.125:8088/			用户名：admin				密码：Harbor12345

mkdir -p /etc/docker/certs.d/reg.longchuang.com
# 写入Harbor服务器证书文件
openssl s_client -showcerts -connect reg.longchuang.com:443 < /dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > /etc/docker/certs.d/reg.longchuang.com/ca.crt

#使用harbor
docker login 10.1.20.125:8088
```

![image-20200803143812024](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghdm6s804kj313m0czta1.jpg)

## 第六章 部署Kafka

##### 环境简介

|     IP      | hostname | broker id |
| :---------: | :------: | :-------: |
| 10.1.20.125 | server6  |     0     |
| 10.1.20.181 | server7  |     1     |
| 10.1.20.243 | server8  |     2     |

##### 部署

```bash
在server6、7、8上都要安装
#下载安装
curl -O https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.5.0/kafka_2.12-2.5.0.tgz
tar xf kafka_2.12-2.5.0.tgz  -C /usr/local/
mv /usr/local/kafka_2.12-2.5.0/ /usr/local/kafka
mkdir -p /home/kafka/zookeeper/{data,logs} /home/kafka/logs
yum install -y java-11-openjdk-devel

#为集群内3台机器创建各自的broker id，server6的id是0，server7的是1,server8的是2，以server6为例，操作命令如下
cat >/home/kafka/zookeeper/data/myid << EOF
0
EOF

#在3台机器上都配置并启动zookeeper(注意：本机ip写成0.0.0.0，以下以server6为例)
vim /usr/local/kafka/config/zookeeper.properties
dataDir=/home/kafka/zookeeper/data
dataLogDir=/home/kafka/zookeeper/logs
initLimit=5
syncLimit=2
server.0=0.0.0.0:2888:3888
server.1=10.1.20.125:2888:3888
server.2=10.1.20.242:2888:3888
/usr/local/kafka/bin/zookeeper-server-start.sh -daemon /usr/local/kafka/config/zookeeper.properties

#3台机器都配置并启动kafka（注意修改broker.id为各自的id，以下配置以server6为例）
vim /usr/local/kafka/config/server.properties
broker.id=0
listeners=PLAINTEXT://10.1.20.125:9092
num.network.threads=8
num.io.threads=5
socket.send.buffer.bytes=1048576
socket.receive.buffer.bytes=1048576
log.dirs=/home/kafka/logs
delete.topic.enable=true
log.cleanup.policy=delete
zookeeper.connect=10.1.20.125:2181,10.1.20.181:2181,10.1.20.242:2181
zookeeper.connection.timeout.ms=6000
max.request.size=5242880
message.max.bytes=5242880
replica.fetch.max.bytes=5342880
num.replica.fetchers=4
/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties

#配置iptables
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 9092 -j ACCEPT -m comment --comment "Kafka"    
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 2181 -j ACCEPT -m comment --comment "Zookeeper"    
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 2888 -j ACCEPT -m comment --comment "Zookeeper"    
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 3888 -j ACCEPT -m comment --comment "Zookeeper"    

#在任意机器上测试结果如下
[root@server7 ~] /usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 3 --topic test2
Created topic test2.
```

## 第七章 K8S集群部署

|     IP      | 主机名  |  角色  |                安装软件                |
| :---------: | :-----: | :----: | :------------------------------------: |
| 10.1.20.125 | server6 | master | kubeadm1.18 、kubelet1.18、kubectl1.18 |
| 10.1.20.181 | server7 | node1  | kubeadm1.18 、kubelet1.18、kubectl1.18 |
| 10.1.20.242 | server8 | node2  | kubeadm1.18 、kubelet1.18、kubectl1.18 |

##### 检查并准备环境

```bash
#k8s安装环境初始化，在server6、7、8上都要执行
1.检查selinux、防火墙、swap
getenforce
systemctl status firewalld
iptables-save

2.检查路由转发
sysctl -p /etc/sysctl.d/kubernetes.conf

3.检查是否开启ipvs模块
lsmod | grep ip_vs

4.修改docker文件驱动
cat /etc/docker/daemon.json

5.查看内核版本
uname -r
```

##### 安装kubernetes1.18

```bash
#在server6、7、8上分别安装kuberneres1.18.0
1.配置yum源
cat>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg  http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum makecache fast

2.安装k8s
yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0

3.创建kubelet kubeadm kubectl的软链接
ln -s /usr/bin/kubeadm /usr/local/bin/kubeadm
ln -s /usr/bin/kubectl /usr/local/bin/kubectl
ln -s /usr/bin/kubelet /usr/local/bin/kubelet
vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd"
systemctl daemon-reload

4.仅在server6上启动kubelet
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
```

##### 部署master节点

```bash
#在主节点server6上用kubeadm初始化，再把server7和server8加入到集群，加入时node节点不要启动kubelet

1、在master节点执行初始化，并记录node加入集群的命令
cat >kubeadm-init.yaml<<EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kubernetesVersion: v1.18.0
networking:
  serviceSubnet: 192.168.0.0/16
  podSubnet: 10.244.0.0/16
EOF
kubeadm init --config=kubeadm-init.yaml|tee kubeadm-init.log

记录信息如下：
kubeadm join 10.1.20.125:6443 --token ef9ei2.rketat03f75x60d8 \
    --discovery-token-ca-cert-hash sha256:3996c349ddb480f91ed7196a7644fc0a9513909545d047db114517860b5eb747

2、配置环境变量
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

3、安装cni网络插件
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

4、绑定权限并使pod能调度到master节点执行
kubectl create clusterrolebinding gitlab-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts --namespace=default
kubectl taint nodes --all node-role.kubernetes.io/master-

5、创建registry-harbor secret对象
docker login -u=admin -p=test reg.longchuang.com
cat ~/.docker/config.json |base64 -w 0						#把计算结果替换到registry-harbor-secret.yml里
cat >registry-harbor-secret.yml<<EOF
apiVersion: v1
kind: Secret
metadata:
  name: registry-harbor
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJyZWcubG9uZ2NodWFuZy5jb20iOiB7CgkJCSJhdXRoIjogIllXUnRhVzQ2ZEdWemRBPT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE4LjA5LjkgKGxpbnV4KSIKCX0KfQ==
EOF
kubectl create -f registry-harbor-secret.yml
```

##### node节点加入集群

```bash
#在node1和node2分别执行如下命令，用来加入集群
kubeadm join 10.1.20.125:6443 --token hfwpy8.zrax1zk1z7v7dw0q  --discovery-token-ca-cert-hash sha256:eaa04fd29d76a3b4aa009e0f6d0e1c5727c0ea85c90f9be9c912d784e1109f42
```

##### 创建k8s-volume卷

```bash
gluster volume create k8s-volume replica 2 transport tcp gls001.longchuang.com:/home/gfs_data2 \ gls002.longchuang.com:/home/gfs_data2 gls003.longchuang.com:/home/gfs_data2 \  gls004.longchuang.com:/home/gfs_data2
gluster volume info
gluster volume start k8s-volume
gluster volume quota k8s-volume enable
gluster volume quota k8s-volume limit-usage / 1TB
gluster volume set k8s-volume performance.cache-size 4GB
gluster volume set k8s-volume performance.io-thread-count 16
gluster volume set k8s-volume network.ping-timeout 10
gluster volume set k8s-volume performance.write-behind-window-size 1024MB
yum install -y glusterfs glusterfs-fuse
```

##### 配置k8s-volume卷

```bash
以下操作均在server6上执行即可

#创建EndPoints资源
cat>glusterfs-endpoints.json<<EOF
{
  "kind": "Endpoints",
  "apiVersion": "v1",
  "metadata": {
    "name": "glusterfs-cluster"
  },
  "subsets": [
    {
      "addresses": [
        {
          "ip": "10.1.20.125"
        },
        {
          "ip": "10.1.20.181"
        },
        {
          "ip": "10.1.20.242"
        },
        {
          "ip": "10.1.20.210"
        }
      ],
      "ports": [
        {
          "port": 24007
        }
      ]
    }
  ]
}
EOF
kubectl apply -f glusterfs-endpoints.json 

#创建server资源
cat>glusterfs-service.json<<EOF
{
  "kind": "Service",
  "apiVersion": "v1",
  "metadata": {
    "name": "glusterfs-cluster"
  },
  "spec": {
    "ports": [
      {"port": 24007}
    ]
  }
}
EOF
kubectl apply -f glusterfs-service.json

#创建PersistentVolume资源
cat>glusterfs-pv.yml<<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gluster-uat-volume
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteMany
  glusterfs:
    endpoints: "glusterfs-cluster"
    path: "k8s-volume"
    readOnly: false
EOF
kubectl apply -f glusterfs-pv.yml

#创建PersistentVolumeClaim资源
cat>glusterfs-pvc.yml<<EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: glusterfs-uat-data
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
EOF
kubectl apply -f glusterfs-pvc.yml
```

## 第八章 部署RabbitMQ微服务

##### 创建k8srabbitvol卷

```bash
1、在GlusterFS集群各节点上分别创建目录
mkdir /home/gfs_rabbitmqdata

2、在主节点执行创建卷
gluster volume create k8srabbitvol replica 2 transport tcp gls001.longchuang.com:/home/gfs_rabbitmqdata_data gls002.longchuang.com:/home/gfs_rabbitmqdata gls003.longchuang.com:/home/gfs_rabbitmqdata gls004.longchuang.com:/home/gfs_rabbitmqdata 
gluster volume start k8srabbitvol

#创建PersistentVolume
cat>glusterfs-rabbit-pv.yml<<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gluster-uat-rabbit-volume
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteMany
  glusterfs:
    endpoints: "glusterfs-cluster"
    path: "k8srabbitvol"
    readOnly: false
EOF
kubectl apply -f glusterfs-rabbit-pv.yml

#创建PersistentVolumeClaim
cat>glusterfs-rabbit-pvc.yml<<EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: glusterfs-uat-rabbitdata
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
EOF
kubectl apply -f glusterfs-rabbit-pvc.yml
```

##### 部署RabbitMQ服务

```bash
#读取longchuang-rabbitmq.yml文件，创建Pod
cat>longchuang-rabbitmq.yml<<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rabbit
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rabbit
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: rabbit
    spec:
      containers:
        -
          env:
            -
              name: RABBITMQ_DEFAULT_USER
              value: rabbit
            -
              name: RABBITMQ_DEFAULT_PASS
              value: test
          image: "rabbitmq:3-management"
          imagePullPolicy: Always
          name: rabbit
          ports:
            -
              containerPort: 15672
              name: rabbit15672
              protocol: TCP
            -
              containerPort: 5672
              name: rabbit5672
              protocol: TCP
          resources: {}
          volumeMounts:
            -
              mountPath: /bitnami
              name: rabbit-persistent-storage
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
        - name: rabbit-persistent-storage
          persistentVolumeClaim:
            claimName: glusterfs-uat-rabbitdata
EOF
kubectl apply -f longchuang-rabbitmq.yml

#读取longchuang-rabbitmq-svc.yml文件(指定selector)，创建Service
cat>longchuang-rabbitmq-svc.yml<<EOF
apiVersion: v1
kind: Service
metadata:
 name: rabbit-service
spec:
 ports:
 - name: rabbit15672
   nodePort: 31199
   port: 15672
   protocol: TCP
   targetPort: 15672
 - name: rabbit5672
   nodePort: 30672
   port: 5672
   protocol: TCP
   targetPort: 5672
 selector:
   app: rabbit
 type: NodePort
EOF
kubectl apply -f longchuang-rabbitmq-svc.yml

#验证结果
1.命令行
[root@server6 ~]# kubectl get pods
NAME                                 READY   STATUS             RESTARTS   AGE
rabbit-76ccbd8dfc-27z87              1/1     Running            0          3h43m
2.页面验证
http://10.2.20.125:31199	账户名: rabbit 	密码: test
```

![image-20200806153050875](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghh4kgczd7j313w08ogmp.jpg)

## 第九章 部署微服务

##### 安装harbor证书

```bash
docker会通过https从harbor上拉取镜像，所以需要在本地安装证书，而k8s调度资源是任意服务器，所以需要在server678都配置

echo "192.168.0.146  reg.longchuang.com" >>/etc/hosts
mkdir -p /etc/docker/certs.d/reg.longchuang.com
openssl s_client -showcerts -connect reg.longchuang.com:443 < /dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > /etc/docker/certs.d/reg.longchuang.com/ca.crt
docker login -u=admin -p=test reg.longchuang.com
```

##### 安装maven3.6

```bash
#需要安装java-11和maven3.6.3来编译代码
yum install java-11-openjdk-devel
wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
tar xf apache-maven-3.6.3-bin.tar.gz -C /usr/local
cd /usr/local
ln -s apache-maven-3.6.3 maven
cat>> /etc/profile<<EOF
export JAVA_HOME=/usr/java/jdk1.8.0_151
export MAVEN_HOME=/usr/local/maven/
export PATH=$PATH:${MAVEN_HOME}/bin:$JAVA_HOME/bin
EOF
source /etc/profile
java -v
mvn -v

#更换maven的源为阿里源
vim /etc/maven/conf/settings.xml
<settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" 
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">
  <mirrors>
    <mirror>
    <id>aliyunmaven</id>
    <mirrorOf>central</mirrorOf>
    <name>aliyun maven</name>
    <url>https://maven.aliyun.com/repository/public </url>
    </mirror>

```

##### 拉取分支工程到本地

```bash
#拉取所有代码，除gateway工程外其他的都从Metro项目组里拉取

git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-edgecomputing.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-foundry-service/longchuang-account.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-edgecomputing.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-edgecomputing.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-edgecomputing-kafka.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-enterprisecustomer.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-foundry-service/longchuang-account.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-authority.git
git clone -b develop git@192.168.0.146:longchuang-metro/metro-config.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-customer.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-edgecomputing.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-edgecomputing-kafka.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-energy.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-enterprise-platform-service/longchuang-enterprisecustomer.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-file.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-foundry-service/longchuang-cloud-gateway.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-hardware.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-mail.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-masterdata.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-mq-consumer.git
git clone git@192.168.0.146:longchuang-metro/metro-operationlog.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-order.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-publish.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-sms.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-software.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-base-platform-service/longchuang-sslc.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-foundry-service/longchuang-base-api.git
git clone git@192.168.0.146:longchuang-metro/metro-enterprise-api.git
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-foundry-service/longchuang-base-api.git
```

##### 修改各工程源文件

```bash
#pom.xml，删除eureka相关内容，并在<dependencies>下添加如下内容
（微服务longchuang-order和longchuang-mq-consumer不需要添加依赖spring-boot-starter-actuator）
（微服务longchuang-edgecomputing-kafka不需要添加kubernetes-netflix、config和netflix-ribbon依赖）
        <dependency>
                <groupId>org.springframework.cloud</groupId>
                <artifactId>spring-cloud-starter-kubernetes-netflix</artifactId>
                <version>0.2.0.RELEASE</version>
        </dependency>
        <dependency>
                <groupId>org.springframework.cloud</groupId>
                <artifactId>spring-cloud-kubernetes-config</artifactId>
                <version>0.2.0.RELEASE</version>
        </dependency>
        <dependency>
                <groupId>org.springframework.cloud</groupId>
                <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
        </dependency>
        <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>

#application里删除eureka，添加调度配置（kafka微服务不用修改）
import org.springframework.scheduling.annotation.EnableScheduling;
@EnableScheduling


#bootstrap.yml：修改连接的数据库、redis;添加配置中心；删除注册中心；添加endpoint(监控相关)
//改服务端口，redis，postgresql的连接地址
server：
  port: 80
//删掉Eureka注册中心，添加config:8888的配置中心（此项目kafka微服务不用配置）
  # 配置中心
  cloud:
    config:
      uri: http://longchuang-config:8888
      fail-fast: true
    kubernetes:
      reload:
        enabled: true
  main:
    allow-bean-definition-overriding: true        

//末尾添加监控相关模块（微服务longchuang-order、longchuang-mq-consumer不需要配置）
management.endpoint.health.show-details: "ALWAYS"
management.endpoints.web.exposure.include: "*"
management:
  endpoint:
    restart:
      enabled: true
    health:
      enabled: true
    info:
      enabled: true

//修改hystrix配置
hystrix:
  command:
    default:
      execution:
        isolation:
          strategy: SEMAPHORE
          thread:
            timeoutInMilliseconds:  120000
          semaphore:
            maxConcurrentRequests: 6000

//修改RabbitMQ配置（微服务longchuang-order，longchuang-mq-consumer需要配置）
    stream:
      binders:
        defaultRabbit:
          type: rabbit
          environment:
            spring:
              rabbitmq:
                host: 10.1.20.125
                port: 30672
                username: rabbit
                password: test
                virtual-host: /
//修改Mail配置（仅微服务longchuang-mail 需要配置）
  mail:
    host: smtp.exmail.qq.com
    port: 465
    username: lcxxx@longchuang.com
    password: xxxx
    default-encoding: UTF-8
    protocol: smtp

# 所有Api工程需要多修改两个文件：ParamsFilter.java（）和SecurityConfig.java文件（configure方法中添加免登录URL）
vi src/main/java/com/longchuang/enterprise/filter/ParamsFilter.java
    @Override
    public void doFilter(ServletRequest arg0, ServletResponse arg1, FilterChain arg2)
            throws IOException, ServletException {
        if (arg0 instanceof  HttpServletRequest) {
            HttpServletRequest httpRequest = (HttpServletRequest) arg0;

            if (httpRequest.getRequestURI().startsWith("/actuator/")) {
                arg2.doFilter(arg0, arg1);
                return;
            }
        }

        ParameterRequestWrapper parmsRequest =
                new ParameterRequestWrapper((HttpServletRequest) arg0);
        arg2.doFilter(parmsRequest, arg1);
    }

vi src/main/java/com/longchuang/enterprise/security/SecurityConfig.java
    @Override
    protected void configure(HttpSecurity http) throws Exception {

        http.csrf().disable().cors().and().exceptionHandling()
                .authenticationEntryPoint(jwtAuthenticationEntryPoint)
                // dont authenticate this particular request
								.and().authorizeRequests()
                .antMatchers("/actuator/**").permitAll()
```

##### 部署config微服务

```bash
#依据修改完的配置文件，编译代码，再打包成镜像，再打标签上传到harhor
cd ~/longchuang-cloud-config
mvn -Dmaven.test.failure.ignore clean package
mkdir m_config/maven -p
cp target/longchuang-cloud-config.jar m_config/maven
cd m_config
cat>Dockerfile<<EOF
FROM openjdk:11-jre
MAINTAINER jakdia
EXPOSE 8888
COPY maven /maven/
CMD java -Dspring.profiles.active="composite" -jar \
/maven/longchuang-cloud-config.jar server \
/maven/docker-config.yml
EOF
cp Dockerfile maven/docker-config.yml
docker build -f Dockerfile -t shentong/longchuang-config .
docker tag shentong/longchuang-config:latest reg.longchuang.com/shentong/longchuang-config:v1.0
docker push reg.longchuang.com/shentong/longchuang-config:v1.0

#生成deployment资源
cd longchuang-cloud-config
cat >longchuang-config.yml<<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: longchuang-config
spec:
  replicas: 1
  selector:
    matchLabels:
      app: longchuang-config
  template:
    metadata:
      labels:
        app: longchuang-config
    spec:
      containers:
      - name: longchuang-config
        image: reg.longchuang.com/shentong/longchuang-config:v1.0
        imagePullPolicy: Always
        env:
          - name: SPRING_PROFILES_ACTIVE
            value: "composite"
        ports:
        - containerPort: 8888
        resources:
          requests:
            cpu: "50m"
            memory: 1Gi
          limits:
            cpu: "2"
            memory: 1Gi
        livenessProbe:
          httpGet:
            scheme: HTTP
            path: /actuator/info
            port: 8888
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 20
          successThreshold: 1
        readinessProbe:
          httpGet:
            scheme: HTTP
            path: /actuator/health
            port: 8888
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 3
          successThreshold: 1
      imagePullSecrets:
      - name: registry-harbor
EOF
kubectl create -f longchuang-config.yml --record

cat>longchuang-config-svc.yml<<EOF
apiVersion: v1
kind: Service
metadata:
  name: longchuang-config
spec:
  selector:
    app: longchuang-config
  ports:
    - port: 8888
EOF
kubectl create -f longchuang-config-svc.yml

#执行结果
[root@server6]# kubectl get pods
NAME                                 READY   STATUS             RESTARTS   AGE
longchuang-config-65785d969b-r9lnc   1/1     Running            0          17m
```

##### 部署gateway微服务

```bash
git clone -b develop git@192.168.0.146:aiotv1.1/longchuang-cloud-foundry-service/longchuang-cloud-gateway.git
mvn -Dmaven.test.failure.ignore clean package
mkdir m_gateway/maven -p
cp target/longchuang-cloud-gateway.jar m_gateway/maven/
cd m_gateway
cat>Dockerfile<<EOF
FROM openjdk:11-jre
MAINTAINER jakdia
EXPOSE 80
COPY maven /maven/
CMD java -Xms700m -Xmx980m -Dspring.profiles.active="docker" -jar \
/maven/longchuang-cloud-gateway.jar server \
/maven/docker-config.yml
EOF
cp Dockerfile  maven/docker-config.yml
docker build -f Dockerfile -t shentong/longchuang-gateway .
docker tag shentong/longchuang-gateway:latest reg.longchuang.com/shentong/longchuang-gateway:v1.0
docker push reg.longchuang.com/shentong/longchuang-gateway:v1.0
cat>longchuang-gateway.yml<<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: longchuang-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: longchuang-gateway
  template:
    metadata:
      labels:
        app: longchuang-gateway
    spec:
      containers:
      - name: longchuang-gateway
        image: reg.longchuang.com/shentong/longchuang-gateway:v1.0
        imagePullPolicy: IfNotPresent
        env:
          - name: SPRING_PROFILES_ACTIVE
            value: "test"
        ports:
        - containerPort: 8002
        resources:
          requests:
            cpu: "50m"
            memory: 1Gi
          limits:
            cpu: "2"
            memory: 1Gi
        livenessProbe:
          httpGet:
            scheme: HTTP
            path: /actuator/info
            port: 80
          initialDelaySeconds: 80
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 20
          successThreshold: 1
        readinessProbe:
          httpGet:
            scheme: HTTP
            path: /actuator/health
            port: 80
          initialDelaySeconds: 80
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 3
          successThreshold: 1
      imagePullSecrets:
      - name: registry-harbor
EOF
cat>longchuang-gateway-svc.yml<<EOF
apiVersion: v1
kind: Service
metadata:
  name: longchuang-gateway
spec:
  type: NodePort
  selector:
    app: longchuang-gateway
  ports:
    - port: 80
      nodePort: 31002
      targetPort: 80
EOF
kubectl create -f longchuang-gateway.yml  --record
kubectl create -f longchuang-gateway-svn.yml
```

##### 部署software微服务

```bash
以software为示例，需要部署的微服务还有account、authority、customer、edgecomputing、edgecomputing、energy、enterprise-web、enterprisecustomer、file、hardware、masterdata、mq-consumer、operationlog、order、publish、sms、sslc、web等
                   

#编译代码
在代码目录内执行编译命令，编译完成后生成target目录
cd ~/longchuang-software
mvn -Dmaven.test.failure.ignore clean package

#制作镜像
mkdir -p m_software/maven
cp target/longchuang-software.jar m_software/maven
cd m_software
cat>Dockerfile<<EOF
FROM openjdk:11-jre
MAINTAINER jakdia
EXPOSE 80
COPY maven /maven/
CMD java -Xms700m -Xmx980m -Dspring.profiles.active="test" -jar \
/maven/longchuang-software.jar server \
/maven/docker-config.yml
EOF
cp Dockerfile maven/docker-config.yml
docker build -f Dockerfile -t shentong/longchuang-software .


#镜像上传到harbor
docker tag shentong/longchuang-software:latest reg.longchuang.com/shentong/longchuang-software:v1.0
docker push reg.longchuang.com/shentong/longchuang-software:v1.0

#创建deploy资源
cat>longchuang-software.yml<<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: longchuang-software
spec:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: longchuang-software
spec:
  replicas: 1
  selector:
    matchLabels:
      app: longchuang-software
  template:
    metadata:
      labels:
        app: longchuang-software
    spec:
      containers:
      - name: longchuang-software
        image: reg.longchuang.com/shentong/longchuang-software:v1.0
        imagePullPolicy: IfNotPresent
        env:
          - name: SPRING_PROFILES_ACTIVE
            value: "test"
        ports:
        - containerPort: 80
        volumeMounts:
        - name: gluster-uat-data-volume
          mountPath: "/data"
        resources:
          requests:
            cpu: "50m"
            memory: 1Gi
          limits:
            cpu: "2"
            memory: 1Gi
      volumes:
      - name: gluster-uat-data-volume
        persistentVolumeClaim:
          claimName: glusterfs-uat-data
      imagePullSecrets:
      - name: registry-harbor
EOF
cat>longchuang-software-svc.yml<<EOF
apiVersion: v1
kind: Service
metadata:
  name: longchuang-software
spec:
  selector:
    app: longchuang-software
  ports:
    - port: 80
      targetPort: 80
EOF
kubectl create -f longchuang-software.yml --record
kubectl create -f longchuang-software-svc.yml
```

##### 部署前端微服务

```bash
#下载前端工程
git clone git@192.168.0.146:longchuang-metro/metro-iot-vue.git
git clone git@192.168.0.146:longchuang-metro/metro-base-vue.git
git clone git@192.168.0.146:longchuang-metro/metro-enterprise-vue.git


#修改2个源文件中后端地址（暂时不用改，忽略）
vim metro-enterprise-vue/src/company/apis/ApiUtil.js
1.暂时不用改，申通项目延用基干平台的后端
let baseUrl = 'http://10.1.20.125:31002' // 其它接口
let loginUrl = 'http://10.1.20.125:31002' // 登录和登出
2.待申通项目后端配置没问题再改成下面的
let baseUrl = 'http://218.202.234.78:47000' // 其它接口
let loginUrl = 'http://218.202.234.78:47001' // 登录和登出

vim metro-enterprise-vue/src/company/SingleSpaConfig.js
        name: 'iot',
        entry: '//10.1.20.125:31006',
        name: 'onlineDevice',
        entry: '//10.1.20.125:8200',
#在本机编译代码,编译完成后enterprise生成dist_company目录，其它项目生成dist目录
1.编译enterprise-vue工程，执行命令
npm install
node build/build.js company

2.iot和base-vue工程执行如下命令
npm install
npm run build

#制作镜像并上传到harbor
mkdir m_entvue
mv dist_company longchuang-front-enterprise-vue
cp -a longchuang-front-enterprise-vue m_entvue
cd m_entvue
cat>Dockerfile<<EOF
FROM nginx
COPY nginx.conf /etc/nginx/nginx.conf
COPY longchuang-front-enterprise-vue /usr/share/nginx/html
EOF

cat>nginx.conf<<EOF
worker_processes 1;
events {
    worker_connections 1024;
}
http {
    include mime.types;
    default_type application/octet-stream;
    sendfile on;
    keepalive_timeout 65;
    server {
        listen 80;
        location = /index.html {
            root /usr/share/nginx/html;
            add_header Cache-Control "no-cache, no-store";
            expires 0;
        }
        location / {
            root /usr/share/nginx/html;
            index index.html index.htm;
            try_files $uri $uri/ @router;
        }
        location @router {
            rewrite ^.*$ /index.html last;
        }
    }
}
EOF
docker build -f Dockerfile -t reg.longchuang.com/shentong/longchuang-front-enterprise-vue:v1.0 .
docker push reg.longchuang.com/shentong/longchuang-front-enterprise-vue:v1.0

#创建deploy资源和svc资源
cat>longchuang-front-enterprise.yml<<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: longchuang-front-enterprise-vue
spec:
  replicas: 1
  selector:
    matchLabels:
      app: longchuang-front-enterprise-vue
  template:
    metadata:
      labels:
        app: longchuang-front-enterprise-vue
    spec:
      containers:
      - name: longchuang-front-enterprise-vue
        image: reg.longchuang.com/shentong/longchuang-front-enterprise-vue:v1.0
        imagePullPolicy: Always
        env:
          - name: SPRING_PROFILES_ACTIVE
            value: "test"
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "50m"
            memory: 1Gi
          limits:
            cpu: "2"
            memory: 1Gi
        livenessProbe:
          httpGet:
            scheme: HTTP
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 20
          successThreshold: 1
        readinessProbe:
          httpGet:
            scheme: HTTP
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 3
          successThreshold: 1
      imagePullSecrets:
      - name: registry-harbor
EOF

cat>longchuang-front-enterprise-vue-svc.yml<<EOF
apiVersion: v1
kind: Service
metadata:
  name: longchuang-front-enterprise-vue
spec:
  type: NodePort
  selector:
    app: longchuang-front-enterprise-vue
  ports:
    - port: 80
      nodePort: 31005
      targetPort: 80
EOF
kubectl create -f longchuang-front-enterprise-vue.yml --record
kubectl create -f longchuang-front-enterprise-vue-svc.yml
```

## 第十章 Troubleshooting

##### mvn打包失败

```bash
#报错1
[ERROR] /from_gitlab/longchuang-sslc/src/main/java/com/longchuang/publish/resources/SslcServeController.java:[26,44] cannot find symbol
[ERROR]   symbol:   class ForbidDeleteException
[ERROR]   location: package com.longchuang.common.core.exception

解决办法
把pom.xml文件中<artifactId>longchuang-common-core</artifactId>里的longchuang改为sto，修改结果为：
<artifactId>sto-common-core</artifactId>

#报错2
[ERROR] Failed to execute goal on project longchuang-customer: Could not resolve dependencies for project com.longchuang:longchuang-customer:jar:0.0.1-SNAPSHOT: Could not find artifact com.longchuang:sto-common-core:jar:0.0.1-SNAPSHOT -> [Help 1]

解决办法：
git clone -b develop git@192.168.0.146:longchuang-metro/merto-common-core.git
cd metro-common-core/
mvn install
最后把该工程的pom.xml中修改longchuang-common-core的引用，改成sto-common-core

#报错3
[ERROR] Failed to execute goal on project longchuang-authority: Could not resolve dependencies for project com.longchuang:longchuang-authority:jar:0.0.1-SNAPSHOT: Failed to collect dependencies at corg.springframework.cloud:spring-cloud-starter-zipkin:jar:2.1.0.RELEASE: Failed to read artifact descriptor for org.springframework.cloud:spring-cloud-starter-zipkin:jar:2.1.0.RELEASE: Could not transfer artifact org.springframework.cloud:spring-cloud-starter-zipkin:pom:2.1.0.RELEASE from/to central (https://repo.maven.apache.org/maven2): Transfer failed for https://repo.maven.apache.org/maven2/org/springframework/cloud/spring-cloud-starter-zipkin/2.1.0.RELEASE/spring-cloud-starter-zipkin-2.1.0.RELEASE.pom: Unknown host repo.maven.apache.org: Name or service not known -> 

解决办法
替换/root/.m2/settings.xml文件
```

##### nodejs编译失败

```bash
#找不到命令
Error: ENOENT: no such file or directory, uv_cwd
解决办法：重启命令行即可

#缺少模块
运行报错Cannot find module '@babel/compat-data/corejs3-shipped-proposals’
npm install @babel/compat-data

#Eslint报错
提示：http://eslint.org/docs/rul*sistent-returnExpected to return a value at the end of arrow function src\company\views\device\AddDeviceModelModal.vue:104:68
解决办法：注释该行，使Eslint不生效,命令如下：
sed -i '/useEslint/s#^#//#g' build/webpack.base.conf.js


#如果直接编译出问题可以考虑换淘宝源
npm install cnpm -g --registry=https://registry.npm.taobao.org
npm config set registry http://registry.npm.taobao.org/
npm get registry
cnpm install 
```

##### 镜像拉取失败

```bash
NAME                                     READY   STATUS    			RESTARTS   AGE
longchuang-account-69d6b599b-b464k       0/1     ErrImagePull   0          6m14s

报错原因：镜像没有从harbor上拉取下来
解决思路：先看这个服务调度到哪台服务器，再到这台服务上手动拉取镜像，看具体报错

kubectl get pod -o wide
[root@server6 longchuang-account]#  docker pull reg.longchuang.com/shentong/longchuang-account:v1.0

v1.0: Pulling from shentong/longchuang-account
Digest: sha256:e5c5d0ffc66f6517d792c11826e4d0f24f11e91bed8cdd74727d093a8569233c
Status: Image is up to date for reg.longchuang.com/shentong/longchuang-account:v1.0
解决：报错因为本地已有最新镜像，需要把longchuang-account.yml文件里imagePullPolicy: Always改为：IfNotPresent
```

##### node节点

```bash
[root@server6 from_gitlab]# kubectl get node
NAME      STATUS     ROLES    AGE     VERSION
server6   NotReady   master   6d20h   v1.18.0
server7   Ready      <none>   6d20h   v1.18.0
server8   Ready      <none>   6d20h   v1.18.0

排查：
kubectl describe node server6
Warning  ImageGCFailed  25m    kubelet, server6  failed to get imageFs info: unable to find data in memory cache

解决
docker system prune
systemctl stop kubelet
systemctl stop docker
systemctl start kubelet
systemctl start docker
kubectl get node
kubectl get pod
```

##### energy工程换数据库

```bash
1.先在server6上用docker起数据库
mkdir /home/mysql-data
docker run --restart unless-stopped -d -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=test -v /home/mysql-data:/var/lib/mysql -d reg.longchuang.com/library/mysql:latest --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci

2.从192.168.50.121的mysql上同步axon库的表结构和数据到10.1.20.125的mysql

3.修改config工程和energy工程的连接数据库为10.1.20.125
sed -i 's#www.buduncle.top#10.1.20.125#g' /from_gitlab/metro-config/target/classes/config/longchuang-energy-test.yml
sed -i 's#www.buduncle.top#10.1.20.125#g' /from_gitlab/longchuang-energy/src/main/resources/bootstrap.yml

4.删除config和energy工程的deploy和svc资源，重新mvn打包上传镜像再生成deploy和svc资源
```

##### 应用发布

```bash
#申通vue项目发布

1.在HbuilderX上执行
git fetch --all
git reset --hard origin/master
sed -i  ''  '/localhost/s#localhost:8100#10.1.20.125:31006#g' src/company/SingleSpaConfig.js
npm install
node build/build.js company
scp -r dist_company root@10.1.20.125:/tmp

2.在server6上执行
cd /longchuang-project/metro-enterprise-vue/longchuang-front-enterprise-vue
rm -fr *
cp -a /tmp/dist_company/* .
cd ..
docker build -f Dockerfile  -t reg.longchuang.com/shentong/longchuang-front-enterprise-vue:v1.0 .
docker push reg.longchuang.com/shentong/longchuang-front-enterprise-vue:v1.0
kubectl delete deploy longchuang-front-enterprise-vue
kubectl delete svc longchuang-front-enterprise-vue
kubectl create -f longchuang-front-enterprise-vue.yml  --record
kubectl create -f longchuang-front-enterprise-vue-svc.yml

3.打开浏览器
http://10.1.20.125:31005/emcs/bigsystem
13700007777  12345678
```































